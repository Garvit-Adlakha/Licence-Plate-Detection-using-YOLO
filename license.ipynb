{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() = False\n",
      "torch.cuda.device_count() = 0\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.cuda.is_available() = }')\n",
    "print(f'{torch.cuda.device_count() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=r\"data\\images\\google_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = dict(\n",
    "   img_path=[],  \n",
    "    xmin=[], \n",
    "    xmax=[], \n",
    "    ymin=[], \n",
    "    ymax=[], \n",
    "    img_w=[], \n",
    "    img_h=[],\n",
    "    name=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_files = glob(f'{dataset_path}/*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               img_path  xmin  xmax  ymin  \\\n",
      "0     data\\images\\google_images\\0073797c-a755-4972-b...   140   339   210   \n",
      "1     data\\images\\google_images\\00b42b2c-f193-4863-b...   184   292   572   \n",
      "2     data\\images\\google_images\\018b52e6-e9a1-42c2-8...   327   399   202   \n",
      "3     data\\images\\google_images\\03273806-bb1e-48da-8...   185   374   290   \n",
      "4     data\\images\\google_images\\0369b20e-b432-4409-9...   335   453   313   \n",
      "...                                                 ...   ...   ...   ...   \n",
      "1684                  data\\images\\google_images\\WB4.jpg   119   191   268   \n",
      "1685                  data\\images\\google_images\\WB5.jpg   105   187   256   \n",
      "1686                  data\\images\\google_images\\WB6.jpg   108   181   247   \n",
      "1687                  data\\images\\google_images\\WB8.jpg    98   184   210   \n",
      "1688                  data\\images\\google_images\\WB9.jpg    92   152   204   \n",
      "\n",
      "      ymax  img_w  img_h        name  \n",
      "0      260    500    335    KA19TR02  \n",
      "1      648   1280    853  TN21AT0492  \n",
      "2      227    660    280  RJ27TC0530  \n",
      "3      339    588    476  MH20CS9817  \n",
      "4      347    480    480  KL05AK3300  \n",
      "...    ...    ...    ...         ...  \n",
      "1684   292    272    363   WB02X8795  \n",
      "1685   280    272    363   WB06G4120  \n",
      "1686   266    272    363  WB02AH4655  \n",
      "1687   229    272    272   WB06J4432  \n",
      "1688   221    272    366  WB02AA5580  \n",
      "\n",
      "[1689 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "for filename in xml_files:\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        info = ET.parse(filename)\n",
    "        root = info.getroot()\n",
    "        \n",
    "        # Extract the image filename and construct the full path\n",
    "        img_name = root.find('filename').text\n",
    "        img_path = os.path.join(dataset_path, img_name)  # Use <path> for full path if available\n",
    "        \n",
    "        # Extract image dimensions from <size>\n",
    "        size_info = root.find('size')\n",
    "        img_w = int(size_info.find('width').text)\n",
    "        img_h = int(size_info.find('height').text)\n",
    "        \n",
    "        # Check if there are any <object> elements\n",
    "        objects_found = False\n",
    "        for member_object in root.findall('object'):\n",
    "            objects_found = True\n",
    "            labels_info = member_object.find('bndbox')\n",
    "            xmin = int(labels_info.find('xmin').text)\n",
    "            ymin = int(labels_info.find('ymin').text)\n",
    "            xmax = int(labels_info.find('xmax').text)\n",
    "            ymax = int(labels_info.find('ymax').text)\n",
    "            name = member_object.find('name').text\n",
    "            \n",
    "            # Append the extracted information to the dictionary\n",
    "            labels_dict['img_path'].append(img_path)\n",
    "            labels_dict['xmin'].append(xmin)\n",
    "            labels_dict['xmax'].append(xmax)\n",
    "            labels_dict['ymin'].append(ymin)\n",
    "            labels_dict['ymax'].append(ymax)\n",
    "            labels_dict['img_w'].append(img_w)\n",
    "            labels_dict['img_h'].append(img_h)\n",
    "            labels_dict['name'].append(name)\n",
    "        \n",
    "        if not objects_found:\n",
    "            print(f\"Warning: No <object> found in {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "alldata = pd.DataFrame(labels_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(alldata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      len(train) = 1182\n",
      "      len(val) = 169\n",
      "      len(test) = 338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(alldata, test_size=2/10, random_state=42)\n",
    "train, val=train_test_split(train,train_size=7/8,random_state=42)\n",
    "print(f'''\n",
    "      len(train) = {len(train)}\n",
    "      len(val) = {len(val)}\n",
    "      len(test) = {len(test)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataframe, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    # Identify classes with only one sample\n",
    "    class_counts = dataframe['name'].value_counts()\n",
    "    single_sample_classes = class_counts[class_counts == 1].index\n",
    "    print(single_sample_classes)\n",
    "    \n",
    "    # Separate single-sample classes from the rest\n",
    "    single_sample_states = dataframe[dataframe['name'].isin(single_sample_classes)]\n",
    "    multi_sample_states = dataframe[~dataframe['name'].isin(single_sample_classes)]\n",
    "    \n",
    "    # Perform stratified split on multi-sample classes\n",
    "    train, temp = train_test_split(\n",
    "        multi_sample_states,\n",
    "        test_size=1 - train_size,\n",
    "        stratify=multi_sample_states['name'].str[:2],  # Stratify by state\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Identify classes with only one sample in temp (for val/test split)\n",
    "    class_counts_val = temp['name'].value_counts()\n",
    "    single_sample_classes_val = class_counts_val[class_counts_val == 1].index\n",
    "    single_sample_states_val = temp[temp['name'].isin(single_sample_classes_val)]\n",
    "    multi_sample_states_val = temp[~temp['name'].isin(single_sample_classes_val)]\n",
    "    \n",
    "    # Split temp into val and test\n",
    "    val, test = train_test_split(\n",
    "        multi_sample_states_val,\n",
    "        test_size=test_size / (val_size + test_size),\n",
    "        stratify=multi_sample_states_val['name'].str[:2],  # Stratify by state\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Add single-sample states directly to the training and validation sets\n",
    "    train = pd.concat([train, single_sample_states], ignore_index=True)\n",
    "    val = pd.concat([val, single_sample_states_val], ignore_index=True)\n",
    "\n",
    "    return {\n",
    "        'train': train,\n",
    "        'val': val,\n",
    "        'test': test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_folder_in_yolo_format(split_name, split_df):\n",
    "    \"\"\"\n",
    "    Creates a folder structure for a dataset split (train/val/test) in YOLO format,\n",
    "    further separating the folders by state based on the first two characters of the 'name' column.\n",
    "\n",
    "    Parameters:\n",
    "    split_name (str): The name of the split (e.g., 'train', 'val', 'test').\n",
    "    split_df (pd.DataFrame): The DataFrame containing the data for the split.\n",
    "\n",
    "    The function will create 'labels' and 'images' subdirectories under\n",
    "    'datasets/cars_license_plate/{split_name}/{state}',\n",
    "    and save the corresponding labels and images in YOLO format.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join('datasets', 'cars_license_plate_new', split_name)\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in split_df.iterrows():\n",
    "        try:\n",
    "            # Extract state from the first two characters of the 'name' column\n",
    "            state = row['name'][:2]\n",
    "            \n",
    "            # Paths for labels and images\n",
    "            state_path = os.path.join(base_path, state)\n",
    "            labels_path = os.path.join(state_path, 'labels')\n",
    "            images_path = os.path.join(state_path, 'images')\n",
    "            \n",
    "            # Create directories for labels and images if they don't exist\n",
    "            os.makedirs(labels_path, exist_ok=True)\n",
    "            os.makedirs(images_path, exist_ok=True)\n",
    "            \n",
    "            # Extract image name and extension\n",
    "            img_name, img_extension = os.path.splitext(os.path.basename(row['img_path']))\n",
    "            \n",
    "            # Calculate YOLO format bounding box coordinates\n",
    "            x_center = (row['xmin'] + row['xmax']) / 2 / row['img_w']\n",
    "            y_center = (row['ymin'] + row['ymax']) / 2 / row['img_h']\n",
    "            width = (row['xmax'] - row['xmin']) / row['img_w']\n",
    "            height = (row['ymax'] - row['ymin']) / row['img_h']\n",
    "\n",
    "            # Save the label in YOLO format\n",
    "            label_path = os.path.join(labels_path, f'{img_name}.txt')\n",
    "            with open(label_path, 'w') as file:\n",
    "                file.write(f\"0 {x_center:.4f} {y_center:.4f} {width:.4f} {height:.4f}\\n\")\n",
    "                \n",
    "            # Check if the image file exists before copying\n",
    "            if os.path.exists(row['img_path']):\n",
    "                shutil.copy(row['img_path'], os.path.join(images_path, img_name + img_extension))\n",
    "            else:\n",
    "                print(f\"Warning: Image file not found - {row['img_path']}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {row['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Created folders under '{base_path}' organized by state.\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `df` is a DataFrame with columns: ['img_path', 'xmin', 'xmax', 'ymin', 'ymax', 'img_w', 'img_h', 'name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'datasets' directory if it exists\n",
    "if os.path.exists('datasets'):\n",
    "    shutil.rmtree('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AP39TP0211', 'AP31TV7236', 'AP31BL8656', 'AP16DP7065', 'AR048395',\n",
      "       'AR152019', 'AR01J0123', 'AR118724', 'AR200692', 'AR01F2523',\n",
      "       ...\n",
      "       'WB06H2237', 'WB02A03017', 'WB02AD4162', 'WB02AM8548', 'WB70G3763',\n",
      "       'WB02X8795', 'WB06G4120', 'WB02AH4655', 'WB06J4432', 'WB02AA5580'],\n",
      "      dtype='object', name='name', length=757)\n",
      "Warning: Image file not found - data\\images\\google_images\\NL1.jpg\n",
      "Created folders under 'datasets\\cars_license_plate_new\\train' organized by state.\n",
      "Created folders under 'datasets\\cars_license_plate_new\\val' organized by state.\n",
      "Created folders under 'datasets\\cars_license_plate_new\\test' organized by state.\n"
     ]
    }
   ],
   "source": [
    "splits = stratified_split(alldata)\n",
    "for split_name, split_df in splits.items():\n",
    "    make_split_folder_in_yolo_format(split_name, split_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders: 37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_folders_in_directory(directory_path):\n",
    "    # List all items in the directory and filter only directories\n",
    "    return len([f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))])\n",
    "\n",
    "directory_path = r'datasets\\cars_license_plate_new\\train'  \n",
    "folder_count = count_folders_in_directory(directory_path)\n",
    "print(f\"Number of folders: {folder_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created AN.yaml in datasets_yaml\n",
      "Created AP.yaml in datasets_yaml\n",
      "Created AR.yaml in datasets_yaml\n",
      "Created AS.yaml in datasets_yaml\n",
      "Created BR.yaml in datasets_yaml\n",
      "Created CG.yaml in datasets_yaml\n",
      "Created CH.yaml in datasets_yaml\n",
      "Created DL.yaml in datasets_yaml\n",
      "Created DN.yaml in datasets_yaml\n",
      "Created DU.yaml in datasets_yaml\n",
      "Created GA.yaml in datasets_yaml\n",
      "Created GJ.yaml in datasets_yaml\n",
      "Created HP.yaml in datasets_yaml\n",
      "Created HR.yaml in datasets_yaml\n",
      "Created JH.yaml in datasets_yaml\n",
      "Created JK.yaml in datasets_yaml\n",
      "Created KA.yaml in datasets_yaml\n",
      "Created KL.yaml in datasets_yaml\n",
      "Created LA.yaml in datasets_yaml\n",
      "Created MH.yaml in datasets_yaml\n",
      "Created ML.yaml in datasets_yaml\n",
      "Created MN.yaml in datasets_yaml\n",
      "Created MP.yaml in datasets_yaml\n",
      "Created MZ.yaml in datasets_yaml\n",
      "Created NL.yaml in datasets_yaml\n",
      "Created OR.yaml in datasets_yaml\n",
      "Created OD.yaml in datasets_yaml\n",
      "Created PB.yaml in datasets_yaml\n",
      "Created PY.yaml in datasets_yaml\n",
      "Created RJ.yaml in datasets_yaml\n",
      "Created SK.yaml in datasets_yaml\n",
      "Created TN.yaml in datasets_yaml\n",
      "Created TR.yaml in datasets_yaml\n",
      "Created TS.yaml in datasets_yaml\n",
      "Created UK.yaml in datasets_yaml\n",
      "Created UP.yaml in datasets_yaml\n",
      "Created WB.yaml in datasets_yaml\n"
     ]
    }
   ],
   "source": [
    "# List of states\n",
    "states = [\n",
    "    'AN',  # Andaman and Nicobar Islands\n",
    "    'AP',  # Andhra Pradesh\n",
    "    'AR',  # Arunachal Pradesh\n",
    "    'AS',  # Assam\n",
    "    'BR',  # Bihar\n",
    "    'CG',  # Chhattisgarh\n",
    "    'CH',  # Chandigarh\n",
    "    'DL',  # Delhi\n",
    "    'DN',  # Dadra and Nagar Haveli and Daman and Diu\n",
    "    'DU',  # (No official RTO code for this - placeholder?)\n",
    "    'GA',  # Goa\n",
    "    'GJ',  # Gujarat\n",
    "    'HP',  # Himachal Pradesh\n",
    "    'HR',  # Haryana\n",
    "    'JH',  # Jharkhand\n",
    "    'JK',  # Jammu and Kashmir\n",
    "    'KA',  # Karnataka\n",
    "    'KL',  # Kerala\n",
    "    'LA',  # Ladakh\n",
    "    'MH',  # Maharashtra\n",
    "    'ML',  # Meghalaya\n",
    "    'MN',  # Manipur\n",
    "    'MP',  # Madhya Pradesh\n",
    "    'MZ',  # Mizoram\n",
    "    'NL',  # Nagaland\n",
    "    'OR',  # Odisha\n",
    "    'OD',  # Odisha (old RTO)\n",
    "    'PB',  # Punjab\n",
    "    'PY',  # Puducherry\n",
    "    'RJ',  # Rajasthan\n",
    "    'SK',  # Sikkim\n",
    "    'TN',  # Tamil Nadu\n",
    "    'TR',  # Tripura\n",
    "    'TS',  # Telangana\n",
    "    'UK',  # Uttarakhand\n",
    "    'UP',  # Uttar Pradesh\n",
    "    'WB',  # West Bengal\n",
    "]\n",
    "\n",
    "# Base path for datasets\n",
    "base_path = \"datasets/cars_license_plate_new\"\n",
    "\n",
    "# Directory to save YAML files\n",
    "yaml_dir = \"datasets_yaml\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(yaml_dir, exist_ok=True)\n",
    "\n",
    "# Class names\n",
    "class_names = ['license_plate']  # Add more classes if applicable.\n",
    "\n",
    "# Function to create YAML file for each state\n",
    "def create_yaml(state):\n",
    "    yaml_content = f\"\"\"\n",
    "# Train and validation image directories\n",
    "train: {base_path}/train/{state}/images\n",
    "val: {base_path}/valid/{state}/images\n",
    "test: {base_path}/test/{state}/images\n",
    "\n",
    "# Train and validation label directories\n",
    "train_labels: {base_path}/train/{state}/labels\n",
    "val_labels: {base_path}/valid/{state}/labels\n",
    "test_labels: {base_path}/test/{state}/labels\n",
    "\n",
    "# Number of classes\n",
    "nc: {len(class_names)}\n",
    "\n",
    "# Class names\n",
    "names: {class_names}\n",
    "\"\"\"\n",
    "    # Write to a YAML file in the specified directory\n",
    "    yaml_path = os.path.join(yaml_dir, f\"{state}.yaml\")\n",
    "    with open(yaml_path, 'w') as file:\n",
    "        file.write(yaml_content.strip())\n",
    "\n",
    "# Generate YAML files for all states\n",
    "for state in states:\n",
    "    create_yaml(state)\n",
    "    print(f\"Created {state}.yaml in {yaml_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
